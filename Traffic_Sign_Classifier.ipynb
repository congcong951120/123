{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages, which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission if necessary. \n",
    "\n",
    "The goal is to modify LeNet to identify German traffic signs in 43 categories. LeNet convolution of two layers, three layers fully connected, I added a layer of convolution, modify the kernel size. Further, the addition of the whole dropou connecting layer for preventing over-fitting. Under this process to build the network of record, summarize method TensorFlow to build and train a network of convolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-68d386b0c78b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.contrib.layers import flatten\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "#training_file = ?\n",
    "#validation_file=?\n",
    "#testing_file = ?\n",
    "\n",
    "training_file = \"traffic-signs-data/train.p\"\n",
    "validation_file= \"traffic-signs-data/valid.p\"\n",
    "testing_file = \"traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n",
    "The training, verification, and test sets each have 34799, 4410, and 12630 pictures, all of which are (32, 32, 3), with a total of 43 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-669632b119e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_valid) == len(y_valid))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "print()\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Training Set:   {} samples\".format(len(y_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_valid)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1e5bbcd6b29e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Data augment.\n",
    "\n",
    "I used two particularly simple methods to expand the data set: one is to turn the pictures to gray, that is, to take the pixels on the three channels as their average; the other is to add one to each picture An integer up to 100.\n",
    "But not all the data was used. Considering that I used CPU training, I only used more than 50,000 pictures.Design and Test a Model Architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ad0c8cb32486>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mData\u001b[0m \u001b[0mAugmen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mn_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 第二种方法，先随机生成X_train.shape[0]个正整数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Augmen\n",
    "\"\"\"\n",
    "n_classes = y_train.max() + 1\n",
    "n_train = X_train.shape[0]\n",
    "# 第二种方法，先随机生成X_train.shape[0]个正整数\n",
    "bias = np.random.randint(0,100,(n_train,1,1,1))\n",
    "# broadcasting，并保证范围都在[0,255]以内\n",
    "X_train_aug0 = np.clip(X_train+bias, 0, 255).astype(np.uint8)\n",
    "y_train_aug0 = y_train\n",
    "\n",
    "# 沿各个图片的channel取均值\n",
    "# 本来为(34799, 32, 32, 3)，取均值后为(34799, 3, 3, 1)\n",
    "a = np.mean(X_train,axis=3,keepdims=True).astype(np.uint8)\n",
    "# 将均值扩到3个channel，(34799, 3, 3, 1)->(34799, 32, 32, 3)\n",
    "X_train_aug1 = np.concatenate((a,a,a),axis=3)\n",
    "y_train_aug1 = y_train\n",
    "\n",
    "X_train_aug = np.concatenate((X_train_aug0,X_train_aug1), axis=0)\n",
    "y_train_aug = np.concatenate((y_train_aug0,y_train_aug1), axis=0)\n",
    "\n",
    "# 保存新数据\n",
    "with open('traffic-signs-data/train_aug.p','wb') as f:\n",
    "    pickle.dump(dict(features=X_train_aug, \n",
    "                     labels=y_train_aug, \n",
    "                     sizes=np.concatenate((train['sizes'],train['sizes']),axis=0),\n",
    "                     coords=np.concatenate((train['coords'],train['coords']),axis=0)), \n",
    "                f)\n",
    "\n",
    "# 添加到训练集，只使用了四分之一的新数据\n",
    "# 现在训练集有52199张图片\n",
    "X_train = np.concatenate((X_train,X_train_aug[0:-1:4,:,:,:]), axis=0)\n",
    "y_train = np.concatenate((y_train,y_train_aug[0:-1:4]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-69de6d8c0f11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# zero out the mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_image_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_train_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrain_image_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# normalize the variance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_image_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# zero out the mean\n",
    "train_image_mean = np.mean(X_train, axis=0)\n",
    "X_train_norm = X_train - train_image_mean\n",
    "# normalize the variance\n",
    "train_image_std = np.std(X_train_norm, axis=0)\n",
    "X_train_norm = X_train_norm / train_image_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-144f15559ef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_valid_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrain_image_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_valid_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtrain_image_std\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_test_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrain_image_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_test_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtrain_image_std\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_valid' is not defined"
     ]
    }
   ],
   "source": [
    "X_valid_norm = X_valid - train_image_mean\n",
    "X_valid_norm = X_valid / train_image_std\n",
    "\n",
    "X_test_norm = X_test - train_image_mean\n",
    "X_test_norm = X_test / train_image_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "sigma = 0.1\n",
    "# trainable用于控制卷积层和全连接层是否训练，记不得当时我为什么要控制这个了......\n",
    "# regularizer是想传入通过tf.contrib.layers.l2_regularizer()返回的函数来着。\n",
    "# 将其用于regularize全连接层参数，但是训练效果不好，最后就没用\n",
    "def initialize_parameters(trainable=[True,True], regularizer=None):\n",
    "    # 将创建好的变量放入字典中返回\n",
    "    parameters = {}\n",
    "    with tf.variable_scope('parameters', reuse=tf.AUTO_REUSE):\n",
    "# Layer 1: Convolutional. \n",
    "        # 每一层都用了variable_scope，并设置了reuse=tf.AUTO_REUSE，配合tf.get_variable()达到共享变量的目标。没有变量时创建，有的话就调用创建好的。\n",
    "        with tf.variable_scope('conv1', reuse=tf.AUTO_REUSE):\n",
    "            # 第一个卷基层的kernel，3x3，输入#channel，输出#channel\n",
    "            conv1_W = tf.get_variable(\"conv1_W\", [3,3,3,16], \n",
    "                              initializer = tf.truncated_normal_initializer(mu,sigma), dtype=tf.float32,\n",
    "                              trainable=trainable[0]) \n",
    "            # 除了kernel以外，别忘了bias\n",
    "            conv1_b = tf.get_variable(\"conv1_b\", [1,1,1,16],\n",
    "                              initializer = tf.zeros_initializer(), dtype=tf.float32,\n",
    "                              trainable=trainable[0])\n",
    "            parameters[\"conv1_W\"] = conv1_W \n",
    "            parameters[\"conv1_b\"] = conv1_b \n",
    "    \n",
    "# Layer 2: Convolutional. \n",
    "        with tf.variable_scope('conv2', reuse=tf.AUTO_REUSE):\n",
    "            conv2_W = tf.get_variable(\"conv2_W\", [3,3,16,32],\n",
    "                              initializer = tf.truncated_normal_initializer(mu,sigma), dtype=tf.float32,\n",
    "                              trainable=trainable[0]) \n",
    "            conv2_b = tf.get_variable(\"conv2_b\", [1,1,1,32],\n",
    "                              initializer = tf.zeros_initializer(), dtype=tf.float32,\n",
    "                              trainable=trainable[0])\n",
    "            parameters[\"conv2_W\"] = conv2_W \n",
    "            parameters[\"conv2_b\"] = conv2_b \n",
    "\n",
    "    \n",
    "# Layer 3: Convolutional.\n",
    "        with tf.variable_scope('conv3', reuse=tf.AUTO_REUSE):\n",
    "            conv3_W = tf.get_variable(\"conv3_W\", [3,3,32,64],\n",
    "                              initializer = tf.truncated_normal_initializer(mu,sigma), dtype=tf.float32,\n",
    "                              trainable=trainable[0]) \n",
    "            conv3_b = tf.get_variable(\"conv3_b\", [1,1,1,64],\n",
    "                              initializer = tf.zeros_initializer(), dtype=tf.float32,\n",
    "                              trainable=trainable[0])\n",
    "    \n",
    "            parameters[\"conv3_W\"] = conv3_W \n",
    "            parameters[\"conv3_b\"] = conv3_b \n",
    "\n",
    "    \n",
    "# Layer 4: Fully Connected. \n",
    "        with tf.variable_scope('fc1', reuse=tf.AUTO_REUSE):\n",
    "            fc1_W = tf.get_variable(\"fc1_W\", [576,120],\n",
    "                            initializer=tf.truncated_normal_initializer(mu,sigma), dtype=tf.float32,\n",
    "                            trainable=trainable[1]) \n",
    "            fc1_b = tf.get_variable(\"fc1_b\", [1,120],\n",
    "                            initializer=tf.zeros_initializer(), dtype=tf.float32,\n",
    "                            trainable=trainable[1])\n",
    "            parameters[\"fc1_W\"] = fc1_W \n",
    "            parameters[\"fc1_b\"] = fc1_b \n",
    "\n",
    "    \n",
    "# Layer 5: Fully Connected. \n",
    "        with tf.variable_scope('fc2', reuse=tf.AUTO_REUSE):\n",
    "            fc2_W = tf.get_variable(\"fc2_W\", [120,84],\n",
    "                            initializer=tf.truncated_normal_initializer(mu,sigma), dtype=tf.float32,\n",
    "                            trainable=trainable[1]) \n",
    "            fc2_b = tf.get_variable(\"fc2_b\", [1,84],\n",
    "                            initializer=tf.zeros_initializer(), dtype=tf.float32,\n",
    "                            trainable=trainable[1])\n",
    "            parameters[\"fc2_W\"] = fc2_W \n",
    "            parameters[\"fc2_b\"] = fc2_b \n",
    "\n",
    "    \n",
    "# Layer 6: Fully Connected. \n",
    "        with tf.variable_scope('fc3', reuse=tf.AUTO_REUSE):\n",
    "            fc3_W = tf.get_variable(\"fc3_W\", [84,n_classes],\n",
    "                            initializer=tf.truncated_normal_initializer(mu,sigma), dtype=tf.float32,\n",
    "                            trainable=trainable[1]) \n",
    "            fc3_b = tf.get_variable(\"fc3_b\", [1,n_classes],\n",
    "                            initializer=tf.zeros_initializer(), dtype=tf.float32,\n",
    "                            trainable=trainable[1])\n",
    "            parameters[\"fc3_W\"] = fc3_W \n",
    "            parameters[\"fc3_b\"] = fc3_b \n",
    "            \n",
    "    # add l2_regularization to collection 'regularize'\n",
    "    # 如果传入了用于normalize的函数，则将对全连接层权值normalize的tensor加入集合'regularize'中。在cost函数中通过集合加入这些regularize项。\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('regularize', regularizer(fc1_W))\n",
    "        tf.add_to_collection('regularize', regularizer(fc2_W))\n",
    "        tf.add_to_collection('regularize', regularizer(fc3_W))\n",
    "    else:\n",
    "        # 如果没有传入regularize函数，则在集合中加个常量0就好。\n",
    "        # 因为后面使用tf.add_n(tf.get_collection('regularize'))来调出集合中的tensor\n",
    "        # 若集合为空则会报错的。\n",
    "        tf.add_to_collection('regularize', tf.constant(0., dtype=tf.float32))\n",
    "    \n",
    "    # 各个可训练的参数都在改字典中了\n",
    "    # 搭建网络结构时调用就好\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Network forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x为建立的placeholder，为(None, 32, 32, 3)\n",
    "# parameters就是上一函数的返回\n",
    "# keep_prob用于控制各层dropout的概率。卷基层概率都设为1。\n",
    "# 必须通过参数控制。\n",
    "# 因为在训练过程中和在测试网络性能过程中其值不同。\n",
    "# 返回的tensor logits为全连接的输出，为(None, 43)，没有接softmax\n",
    "# 因为后面使用的是tf.nn.softmax_cross_entropy_with_logits()来构造cost\n",
    "def LeNet_forwardpass(x, parameters, keep_prob):    \n",
    "# Layer 1: Convolutional. Input = 32x32x3. Output = 32x32x16.\n",
    "    conv1_W = parameters[\"conv1_W\"]\n",
    "    conv1_b = parameters[\"conv1_b\"]\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1,1,1,1], padding='SAME') + conv1_b\n",
    "    # Activation.\n",
    "    conv1   = tf.nn.leaky_relu(conv1, name=\"conv1_out\")\n",
    "\n",
    "    # Pooling. Input = 32x32x16. Output = 16x16x16.\n",
    "    conv1   = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # dropout\n",
    "    conv1   = tf.nn.dropout(conv1, keep_prob=keep_prob[0])\n",
    "\n",
    "# Layer 2: Convolutional. Input = 16x16x16. Output = 16x16x32.\n",
    "    conv2_W = parameters[\"conv2_W\"]\n",
    "    conv2_b = parameters[\"conv2_b\"]\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1,1,1,1], padding='SAME') + conv2_b\n",
    "    \n",
    "    # Activation.\n",
    "    conv2   = tf.nn.leaky_relu(conv2, name=\"conv2_out\")\n",
    "\n",
    "    # Pooling. Input = 16x16x32. Output = 8x8x32.\n",
    "    conv2   = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # dropout\n",
    "    conv2   = tf.nn.dropout(conv2, keep_prob=keep_prob[1])\n",
    "\n",
    "# Layer 3: Convolutional. Input = 8x8x32. Output = 8x8x64.\n",
    "    conv3_W = parameters[\"conv3_W\"]\n",
    "    conv3_b = parameters[\"conv3_b\"]\n",
    "    conv3   = tf.nn.conv2d(conv2, conv3_W, strides=[1,1,1,1], padding='SAME') + conv3_b\n",
    "    \n",
    "    # Activation.\n",
    "    conv3   = tf.nn.leaky_relu(conv3, name=\"conv3_out\")\n",
    "    \n",
    "    # Pooling. Input = 8x8x64. Output = 3x3x64.\n",
    "    conv3   = tf.nn.max_pool(conv3, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # dropout\n",
    "    conv3   = tf.nn.dropout(conv3, keep_prob=keep_prob[2])\n",
    "\n",
    "# Flatten. Input = 3x3x64. Output = 576.\n",
    "    fc0     = flatten(conv3)\n",
    "    \n",
    "# Layer 4: Fully Connected. Input = 576. Output = 120.\n",
    "    fc1_W = parameters[\"fc1_W\"]\n",
    "    fc1_b = parameters[\"fc1_b\"]\n",
    "    fc1     = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc1     = tf.nn.leaky_relu(fc1, name=\"fc1_out\")\n",
    "    # Dropout\n",
    "    fc1     = tf.nn.dropout(fc1, keep_prob=keep_prob[3])\n",
    "\n",
    "# Layer 5: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W = parameters[\"fc2_W\"]\n",
    "    fc2_b = parameters[\"fc2_b\"]\n",
    "    fc2   = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc2     = tf.nn.leaky_relu(fc2, name=\"fc2_out\")\n",
    "    # Dropout\n",
    "    fc2     = tf.nn.dropout(fc2, keep_prob=keep_prob[4])\n",
    "\n",
    "# Layer 6: Fully Connected. Input = 84. Output = n_classes.\n",
    "    fc3_W = parameters[\"fc3_W\"]\n",
    "    fc3_b = parameters[\"fc3_b\"]\n",
    "    logits  = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create input and output placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=(None, 32, 32, 3))\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None))\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, shape=(5))\n",
    "# one_hot_y为(None, 43)，每一行只有一个1，其他为0.\n",
    "one_hot_y = tf.one_hot(y, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: cost and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-e016a40f4070>:20: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\ProgramData1\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001BD17359848>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001BD17359848>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001BD17359848>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001BD17359848>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From <ipython-input-10-242e32e25edc>:24: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练的epoch和batch size\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 初始学习率\n",
    "rate = 0.005\n",
    "# 下降率\n",
    "decay_rate = 0.8\n",
    "# 多少次一下降\n",
    "decay_steps = 500\n",
    "trainable = [True,True] \n",
    "lamb = 0.01\n",
    "\n",
    "# 返回的regularizer是一个函数，可以对输入tensor计算正则项\n",
    "regularizer = tf.contrib.layers.l2_regularizer(lamb)\n",
    "\n",
    "# 调用上面的函数建立参数\n",
    "parameters = initialize_parameters(trainable, regularizer=None)\n",
    "# 前向传播得到输出\n",
    "logits = LeNet_forwardpass(x, parameters, keep_prob)\n",
    "# 对网络的输出softmax后与label计算cross entropy\n",
    "# 返回的是各个example的cross entropy，下面要对其求平均\n",
    "# 该API不再推荐了，推荐使用v2版本\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "\n",
    "# 求平均后，加上regularization项，但这里别没有\n",
    "loss_operation = tf.reduce_mean(cross_entropy) + tf.add_n(tf.get_collection('regularize'))\n",
    "\n",
    "# learning rate decay\n",
    "# 这个global_step是一个不可训练的变量，需要传给优化器对象的minimize方法\n",
    "# 在训练时，每一次优化都会对其加1\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "# 返回的为tensor，可看文档，是对初始学习率decay的学习率tensor。\n",
    "decay_rate = tf.train.exponential_decay(decay_rate=decay_rate, decay_steps=decay_steps, \n",
    "                                        global_step=global_step, learning_rate=rate)\n",
    "\n",
    "# 建立优化器对象。learning_rate参数可传入浮点数或tensor\n",
    "# 这里就传入的tensor\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = decay_rate)\n",
    "# trainig_operation是一个Operation，指出优化的是loss_operation.\n",
    "# 文档中也说了：If global_step was not None, that operation also increments global_step.\n",
    "# 现在传入了global_step，那么每次执行该Operation就会对global_step加1.\n",
    "training_operation = optimizer.minimize(loss_operation, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 很简单，accuracy_operation也就是准去率tensor了\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1, output_type=tf.int32), y)\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 建立个函数在Session中调用，方便\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    # 这里是按batch评估准确率，最后计算总准确率了\n",
    "    # 这样的话应该运行速度比一次计算整体数据集的准确率快。\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        # 因为是测试模型准确率，所以dropout概率为1，就是没有dropout。\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: [1.0,1.0,1.0,1.0,1.0]})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "# 建立各Saver对象用于训练完成后保存模型。\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Train Accuracy = 0.88366\n",
      "Validation Accuracy = 0.78118\n",
      "\n",
      "EPOCH 2 ...\n",
      "Train Accuracy = 0.95944\n",
      "Validation Accuracy = 0.87574\n",
      "\n",
      "EPOCH 3 ...\n",
      "Train Accuracy = 0.97726\n",
      "Validation Accuracy = 0.93016\n",
      "\n",
      "EPOCH 4 ...\n",
      "Train Accuracy = 0.98866\n",
      "Validation Accuracy = 0.93764\n",
      "\n",
      "EPOCH 5 ...\n",
      "Train Accuracy = 0.98935\n",
      "Validation Accuracy = 0.95510\n",
      "\n",
      "EPOCH 6 ...\n",
      "Train Accuracy = 0.99167\n",
      "Validation Accuracy = 0.93946\n",
      "\n",
      "EPOCH 7 ...\n",
      "Train Accuracy = 0.99452\n",
      "Validation Accuracy = 0.94354\n",
      "\n",
      "EPOCH 8 ...\n",
      "Train Accuracy = 0.99697\n",
      "Validation Accuracy = 0.95351\n",
      "\n",
      "EPOCH 9 ...\n",
      "Train Accuracy = 0.99787\n",
      "Validation Accuracy = 0.95510\n",
      "\n",
      "EPOCH 10 ...\n",
      "Train Accuracy = 0.99787\n",
      "Validation Accuracy = 0.96327\n",
      "\n",
      "EPOCH 11 ...\n",
      "Train Accuracy = 0.99881\n",
      "Validation Accuracy = 0.96281\n",
      "\n",
      "EPOCH 12 ...\n",
      "Train Accuracy = 0.99902\n",
      "Validation Accuracy = 0.96417\n",
      "\n",
      "EPOCH 13 ...\n",
      "Train Accuracy = 0.99933\n",
      "Validation Accuracy = 0.96531\n",
      "\n",
      "EPOCH 14 ...\n",
      "Train Accuracy = 0.99933\n",
      "Validation Accuracy = 0.96939\n",
      "\n",
      "EPOCH 15 ...\n",
      "Train Accuracy = 0.99958\n",
      "Validation Accuracy = 0.96463\n",
      "\n",
      "EPOCH 16 ...\n",
      "Train Accuracy = 0.99944\n",
      "Validation Accuracy = 0.96417\n",
      "\n",
      "EPOCH 17 ...\n",
      "Train Accuracy = 0.99973\n",
      "Validation Accuracy = 0.96735\n",
      "\n",
      "EPOCH 18 ...\n",
      "Train Accuracy = 0.99977\n",
      "Validation Accuracy = 0.96848\n",
      "\n",
      "EPOCH 19 ...\n",
      "Train Accuracy = 0.99979\n",
      "Validation Accuracy = 0.96621\n",
      "\n",
      "EPOCH 20 ...\n",
      "Train Accuracy = 0.99987\n",
      "Validation Accuracy = 0.97143\n",
      "\n",
      "EPOCH 21 ...\n",
      "Train Accuracy = 0.99989\n",
      "Validation Accuracy = 0.97324\n",
      "\n",
      "EPOCH 22 ...\n",
      "Train Accuracy = 0.99992\n",
      "Validation Accuracy = 0.96893\n",
      "\n",
      "EPOCH 23 ...\n",
      "Train Accuracy = 0.99992\n",
      "Validation Accuracy = 0.97120\n",
      "\n",
      "EPOCH 24 ...\n",
      "Train Accuracy = 0.99992\n",
      "Validation Accuracy = 0.97007\n",
      "\n",
      "EPOCH 25 ...\n",
      "Train Accuracy = 0.99994\n",
      "Validation Accuracy = 0.96961\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# 用于在每次epoch中shuffle数据集\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# 记录每次训练后batch上的cost值\n",
    "cost = []\n",
    "# 记录训练完一个epoch后，在整个数据集上的cost。\n",
    "cost_epoch = []\n",
    "# keep_prob\n",
    "# 训练时dropout的概率，卷积层不dropout\n",
    "k = [1.,1.,1.,0.4,0.5]\n",
    "with tf.Session() as sess:\n",
    "    # 可别忘了初始化全局变量\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train_norm)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        # 这里shuffle的数据集。感觉弄一个数据集的索引列表，\n",
    "        # shuffle该列表应该快一些。\n",
    "        X_train_norm, y_train = shuffle(X_train_norm, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            # 在numpy中不用担心索引超出数组边界。\n",
    "            batch_x, batch_y = X_train_norm[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: k})\n",
    "            loss = sess.run(loss_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: [1.0,1.0,1.0,1.0,1.0]})\n",
    "            cost.append(loss)\n",
    "        \n",
    "        loss = sess.run(loss_operation, feed_dict={x: X_train_norm, y: y_train, keep_prob: [1.0,1.0,1.0,1.0,1.0]})\n",
    "        cost_epoch.append(loss)\n",
    "        \n",
    "        # 分别在训练、测试集上评估正确率。\n",
    "        train_accuracy = evaluate(X_train_norm, y_train)    \n",
    "        validation_accuracy = evaluate(X_valid_norm, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Train Accuracy = {:.5f}\".format(train_accuracy))\n",
    "        print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    # 保存训练好的模型\n",
    "    saver.save(sess, './model/model.ckpt')\n",
    "    print(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Evaluation test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData1\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n",
      "Test Accuracy = 0.945\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # restore的过程就相当于将保存的训练好的模型加载到当前graph中\n",
    "    # 构建好的结构中。在这一过程中，各变量的值都被初始化为保存好的\n",
    "    # 变量的值。\n",
    "    # 也就不需要再调用sess.run(tf.global_variables_initializer())了\n",
    "    saver.restore(sess, './model/model.ckpt')\n",
    "\n",
    "    test_accuracy = evaluate(X_test_norm, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
